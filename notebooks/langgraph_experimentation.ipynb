{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "444c5dd3-6d4d-4977-9efa-6ca40ed7bd48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langgraph\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fd51d32c-4692-4a4f-8a1a-e0d7ffdb6e6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Good morning! How can I assist you today?', response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 9, 'total_tokens': 19}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-81848ead-b46e-4bda-9166-a90ec4047dab-0')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    temperature=0\n",
    ")\n",
    "model.invoke(\"Good morning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a9805d7e-40d9-4435-9aa6-c34e25b11e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Sequence\n",
    "import operator\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: BaseMessage\n",
    "    code_generated: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    critique_generated: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    explanation_generated: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    is_explanation_generated: False\n",
    "    critique: False\n",
    "    execute_code: False\n",
    "    error_occurred: False\n",
    "    exception_error: Annotated[Sequence[BaseMessage], operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9442965f-a286-4ce4-8ddf-ac05a216b7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coder_node(state):\n",
    "    last_message = state[\"messages\"]\n",
    "    last_code = state[\"code_generated\"] \n",
    "    last_critique = state[\"critique_generated\"]\n",
    "    if state[\"critique\"]:\n",
    "        enhanced_prompt = f\"\"\"Here is the code: {last_code}. Here is the critique: {last_critique[-1]}.\n",
    "        Make modifications to the code based on the critique. ALWAYS ensure to ONLY write code without any additional\n",
    "        information. The code should ALWAYS start with ```\"\"\"\n",
    "    else:\n",
    "        enhanced_prompt = f\"\"\"Here is the user query: {last_message}. Write a code in\n",
    "        as much detail as possible. ALWAYS ensure to ONLY write code without any additional\n",
    "        information. The code should ALWAYS start with ```\"\"\"\n",
    "    response = model.invoke(enhanced_prompt)\n",
    "    return {\"code_generated\": [response.content]}\n",
    "\n",
    "def code_review_node(state):\n",
    "    messages = state[\"code_generated\"]\n",
    "    last_code = messages[-1]\n",
    "    enhanced_prompt = f\"\"\"Based on the results from the code, check and \n",
    "    critique this code to find out if there are more things to add and there are no mistakes at all: {last_code}\"\"\"\n",
    "    response = model.invoke(enhanced_prompt)\n",
    "    return {\"critique_generated\": [response.content], \"critique\": True}\n",
    "\n",
    "def executor_node(state):\n",
    "    if not state[\"execute_code\"]:\n",
    "        final_code = state[\"code_generated\"][-1]\n",
    "        code_blocks = re.findall(r\"```(python)?(.*?)```\", final_code, re.DOTALL)\n",
    "        code = \"\\n\".join([block[1].strip() for block in code_blocks])\n",
    "        if code:\n",
    "            with open(\"llm_generated_code.py\", \"w\") as file:\n",
    "                file.write(code)\n",
    "            try:\n",
    "                with open(\"llm_generated_code.py\", \"r\") as file:\n",
    "                    exec(file.read(), {})\n",
    "            except Exception as e:\n",
    "                return {\"error_occurred\": True, \"exception_error\": f\"{e}\"}\n",
    "        return {\"execute_code\": True, \"error_occurred\": False}\n",
    "\n",
    "def debugger_node(state):\n",
    "    if state[\"error_occurred\"]:\n",
    "        messages = state[\"code_generated\"]\n",
    "        last_code = messages[-1]\n",
    "        exception_error_generated = state[\"exception_error\"][-1]\n",
    "        enhanced_prompt = f\"\"\"Here is the code: {last_code}. Now \n",
    "        this code ran into the following error: {exception_error_generated}.\n",
    "        Correct the code and try again.\n",
    "        ALWAYS ensure to ONLY write code without any additional\n",
    "        information. The code should ALWAYS start with ```\n",
    "        \"\"\"\n",
    "        response = model.invoke(enhanced_prompt)\n",
    "        return {\"code_generated\": [response.content]                                                                                     }\n",
    "\n",
    "def explainer_node(state):\n",
    "    messages = state[\"code_generated\"]\n",
    "    last_code = messages[-1]\n",
    "    enhanced_prompt = f\"\"\"Here is the code: {last_code}. \n",
    "    Based on the code generated, explain not the code\n",
    "    but the business impact about what the code is doing.\n",
    "    It should not be code explanation but explanation in \n",
    "    terms of business value it is giving.\n",
    "    \"\"\"\n",
    "    response = model.invoke(enhanced_prompt)\n",
    "    return {\"explanation_generated\": [response.content], \"is_explanation_generated\": True}\n",
    "        \n",
    "def should_continue(state):\n",
    "    if len(state[\"code_generated\"]) < 2:\n",
    "        return \"code_review\"\n",
    "    else:\n",
    "        return \"executor\"\n",
    "\n",
    "def should_continue_executor(state):\n",
    "    if state[\"execute_code\"] == True and state[\"error_occurred\"] == False:\n",
    "        return \"explainer\"\n",
    "    if state[\"execute_code\"] == True and state[\"error_occurred\"] == True:\n",
    "        return \"debugger\"\n",
    "\n",
    "def should_continue_debugger(state):\n",
    "    if state[\"error_occurred\"]:\n",
    "        return \"executor\"\n",
    "    else:\n",
    "        return \"explainer\"\n",
    "\n",
    "def should_continue_explainer(state):\n",
    "    if state[\"execute_code\"] == True and state[\"error_occurred\"] == False:\n",
    "        return \"explainer\"\n",
    "    if state[\"execute_code\"] == True and state[\"error_occurred\"] == True and state[\"is_explanation_generated\"] == True:\n",
    "        return END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "844acad5-1f13-4c6e-bb8c-2739af51c7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "workflow.add_node(\"coder\", coder_node)\n",
    "workflow.add_node(\"code_review\", code_review_node)\n",
    "workflow.add_node(\"executor\", executor_node)\n",
    "workflow.add_node(\"debugger\", debugger_node)\n",
    "workflow.add_node(\"explainer\", explainer_node)\n",
    "\n",
    "workflow.add_conditional_edges(\"coder\", should_continue)\n",
    "workflow.add_conditional_edges(\"executor\", should_continue_executor)\n",
    "workflow.add_conditional_edges(\"debugger\", should_continue_debugger)\n",
    "workflow.add_edge(\"explainer\", END)\n",
    "workflow.add_edge(\"code_review\", \"coder\")\n",
    "workflow.set_entry_point(\"coder\")\n",
    "# workflow.set_finish_point(\"explainer\")\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fc116d20-0790-4d26-be13-b0a02a8a7a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:6: FutureWarning:\n",
      "\n",
      "'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "\n",
      "Skipping write for channel None:inbox which has no readers\n"
     ]
    }
   ],
   "source": [
    "input_prompt = \"\"\"\n",
    "Build a time series plot of a random dataset and also give explanation \n",
    "of the time series. Ensure that the date lies between 2018 to 2023. It \n",
    "should be about Computer sales considering all the conditions that actually \n",
    "occurred in the world. Use plotly to display the visuals in jupyter notebook\n",
    "\"\"\"\n",
    "\n",
    "input = {\"messages\": input_prompt}\n",
    "response = app.invoke(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e1e9ce-8be7-4a2f-9484-4d6d5c0bfb0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
